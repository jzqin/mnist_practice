{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from code import helper_functions as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/jqin/Documents/Adulting/Studying/mnist_practice/data'\n",
    "train_data_file = 'train-images.idx3-ubyte'\n",
    "train_labels_file = 'train-labels.idx1-ubyte'\n",
    "test_data_file = 't10k-images.idx3-ubyte'\n",
    "test_labels_file = 't10k-labels.idx1-ubyte'\n",
    "\n",
    "train_data, train_labels = hf.load_data(data_dir, train_data_file, train_labels_file)\n",
    "test_data, test_labels = hf.load_data(data_dir, test_data_file, test_labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_feat, n_hidden, n_out, n_heads, n_layer):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(n_feat, n_hidden) \n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(n_hidden, nhead=n_heads), num_layers=n_layer)\n",
    "        self.ffn = nn.Linear(n_hidden*n_feat, n_out)\n",
    "        self.output_fn = nn.LogSoftmax(dim=0)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        batch_size = data.shape[0]\n",
    "        embedded = self.embeddings(data)\n",
    "        encoded = torch.reshape(self.encoder(embedded), (batch_size, -1))\n",
    "        output = self.ffn(encoded)\n",
    "        preds = self.output_fn(output)\n",
    "        \n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feat = 28*28\n",
    "n_hidden = 30\n",
    "n_out = 10\n",
    "n_heads = 5\n",
    "n_layer = 3\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = Transformer(n_feat, n_hidden, n_out, n_heads, n_layer)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0 is 2.327805995941162\n",
      "Loss on epoch 10 is 1.2722746133804321\n",
      "Loss on epoch 20 is 0.996909499168396\n",
      "Loss on epoch 30 is 0.9750736951828003\n",
      "Loss on epoch 40 is 0.6945288777351379\n",
      "Loss on epoch 50 is 1.5724852085113525\n",
      "Loss on epoch 60 is 0.7617841362953186\n",
      "Loss on epoch 70 is 1.0861659049987793\n",
      "Loss on epoch 80 is 0.7380486726760864\n",
      "Loss on epoch 90 is 1.2996108531951904\n",
      "Loss on epoch 100 is 0.8445766568183899\n",
      "Loss on epoch 110 is 0.7441269159317017\n",
      "Loss on epoch 120 is 0.8465034365653992\n",
      "Loss on epoch 130 is 1.1179803609848022\n",
      "Loss on epoch 140 is 0.8090758323669434\n",
      "Loss on epoch 150 is 0.7708579301834106\n",
      "Loss on epoch 160 is 1.0725456476211548\n",
      "Loss on epoch 170 is 1.0307934284210205\n",
      "Loss on epoch 180 is 0.6791755557060242\n",
      "Loss on epoch 190 is 0.7750086784362793\n",
      "Loss on epoch 200 is 0.38142916560173035\n",
      "Loss on epoch 210 is 0.8576698303222656\n",
      "Loss on epoch 220 is 1.5651919841766357\n",
      "Loss on epoch 230 is 1.218719244003296\n",
      "Loss on epoch 240 is 1.3919713497161865\n",
      "Loss on epoch 250 is 0.5803241729736328\n",
      "Loss on epoch 260 is 0.30033165216445923\n",
      "Loss on epoch 270 is 1.0403181314468384\n",
      "Loss on epoch 280 is 0.8605650067329407\n",
      "Loss on epoch 290 is 0.7544766664505005\n",
      "Loss on epoch 300 is 0.7978081703186035\n",
      "Loss on epoch 310 is 0.8143369555473328\n",
      "Loss on epoch 320 is 1.7189384698867798\n",
      "Loss on epoch 330 is 0.9923949241638184\n",
      "Loss on epoch 340 is 0.8962022662162781\n",
      "Loss on epoch 350 is 0.7735384106636047\n",
      "Loss on epoch 360 is 0.695599377155304\n",
      "Loss on epoch 370 is 0.3486263155937195\n",
      "Loss on epoch 380 is 0.7763916254043579\n",
      "Loss on epoch 390 is 0.5343857407569885\n",
      "Loss on epoch 400 is 0.6227626800537109\n"
     ]
    }
   ],
   "source": [
    "n_train = train_labels.shape[0]\n",
    "batch_size = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(401):\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    start_idx = (epoch * batch_size) % n_train\n",
    "    end_idx = min(n_train, start_idx + batch_size)\n",
    "    batch_data = torch.tensor(train_data[start_idx:end_idx]).to(torch.int64)\n",
    "    batch_labels = torch.tensor(train_labels[start_idx:end_idx]).to(torch.int64)\n",
    "\n",
    "    preds = model(batch_data)\n",
    "    loss = loss_fn(preds, batch_labels)\n",
    "    \n",
    "    if (epoch % 10 == 0):\n",
    "        print('Loss on epoch {} is {}'.format(epoch, torch.sum(loss)))\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.tensor(test_data).to(torch.int64)\n",
    "test_labels = torch.tensor(train_labels).to(torch.int64)\n",
    "\n",
    "test_preds = model(test_data[0:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
